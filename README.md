# ESILV Smart Assistant

Assistant intelligent pour l'ESILV utilisant le scraping web, la recherche vectorielle (RAG) et Google Vertex AI.

## ğŸš€ Installation et Configuration (PremiÃ¨re fois)

### PrÃ©requis

- Python 3.9 ou supÃ©rieur
- Un compte Google Cloud Platform (GCP) avec Vertex AI activÃ©
- Les credentials GCP (fichier JSON de service account)

### 1. Cloner le projet

```bash
git clone <url-du-repo>
cd ESILV-Smart-assistant
```

### 2. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 3. Configuration de Google Vertex AI

#### a) CrÃ©er un projet GCP et activer Vertex AI

1. Aller sur [Google Cloud Console](https://console.cloud.google.com/)
2. CrÃ©er un nouveau projet ou sÃ©lectionner un projet existant
3. Activer l'API Vertex AI :
   - Aller dans "APIs & Services" > "Enable APIs and Services"
   - Rechercher "Vertex AI API" et l'activer

#### b) CrÃ©er un service account et tÃ©lÃ©charger les credentials

1. Aller dans "IAM & Admin" > "Service Accounts"
2. Cliquer sur "Create Service Account"
3. Donner un nom (ex: `esilv-smart-assistant`)
4. Attribuer les rÃ´les :
   - `Vertex AI User`
   - `Storage Object Viewer` (si besoin)
5. CrÃ©er une clÃ© JSON :
   - Cliquer sur le service account crÃ©Ã©
   - Onglet "Keys" > "Add Key" > "Create new key"
   - Choisir le format JSON
   - Le fichier JSON sera tÃ©lÃ©chargÃ© automatiquement
6. Placer le fichier JSON dans `Back/app/` (ex: `Back/app/esilv-smart-assistant-xxxxx.json`)

### 4. Configuration de l'environnement (.env)

CrÃ©er un fichier `.env` Ã  la racine du projet avec le contenu suivant :

```env
# Configuration du scraping
SCRAPING_URL=https://www.esilv.fr/

# Configuration de la base de donnÃ©es vectorielle
CHROMA_DB_PATH=./data/chroma_db

# Configuration du RAG
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

SYSTEM_PROMPT="Tu es un assistant pour l'ecole d'ingenieurs ESILV. Reponds aux questions en utilisant le contexte fourni. Si l'information n'est pas presente dans le contexte mais que tu penses qu'elle pourrait se trouver sur le site ESILV, suggere a l'utilisateur de consulter directement le site web https://www.esilv.fr ou indique-lui les pages pertinentes a visiter. Reponds toujours en francais et de maniere claire et concise."

# Vertex AI Configuration (SDK)
GOOGLE_APPLICATION_CREDENTIALS=C:\chemin\absolu\vers\votre\fichier.json
VERTEX_MODEL=gemini-2.0-flash-exp
VERTEX_PROJECT=votre-project-id-gcp
VERTEX_LOCATION=us-central1

# Admin Panel Authentication
ADMIN_PASSWORD=admin2025
```

**âš ï¸ Important :** 
- Remplacer `GOOGLE_APPLICATION_CREDENTIALS` par le chemin ABSOLU vers votre fichier JSON de credentials
- Remplacer `VERTEX_PROJECT` par votre Project ID GCP
- Le fichier `.env` est ignorÃ© par git pour la sÃ©curitÃ©

### 5. Scraper le site (premiÃ¨re fois)

Cette Ã©tape va rÃ©cupÃ©rer tout le contenu du site ESILV :

```bash
python Back/app/rag/scraper.py
```

Cette commande va :
- Scraper jusqu'Ã  500 pages du site ESILV
- Sauvegarder les donnÃ©es dans `data/scraped_data.json`
- Prendre environ 5-10 minutes selon la vitesse de connexion
- CrÃ©er une sauvegarde dans `data/archive_YYYYMMDD_HHMMSS/`

**Note :** Les donnÃ©es scrapÃ©es sont sauvegardÃ©es localement et ne sont pas versionnÃ©es dans git.

### 6. Indexer les donnÃ©es (premiÃ¨re fois)

Cette Ã©tape va crÃ©er l'index de recherche vectorielle :

```bash
python Back/app/admin_indexer.py
```

Cette commande va :
- Charger les donnÃ©es de `data/scraped_data.json`
- DÃ©couper le contenu en chunks optimisÃ©s
- CrÃ©er les embeddings vectoriels avec le modÃ¨le `paraphrase-multilingual-MiniLM-L12-v2`
- GÃ©nÃ©rer l'index FAISS dans `data/faiss_index.bin`
- Sauvegarder le mapping dans `data/faiss_mapping.json`
- Prendre environ 2-5 minutes selon la quantitÃ© de donnÃ©es

### 7. Lancer l'application

#### Interface utilisateur (Streamlit)

```bash
cd Front
streamlit run streamlit_app.py
```

L'application sera accessible sur `http://localhost:8501`

## ğŸ“ Structure du projet

```
ESILV-Smart-assistant/
â”œâ”€â”€ .env                          # Configuration (Ã  crÃ©er - ignorÃ© par git)
â”œâ”€â”€ config.py                     # Gestion centralisÃ©e des chemins
â”œâ”€â”€ requirements.txt              # DÃ©pendances Python
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/                         # DonnÃ©es gÃ©nÃ©rÃ©es (ignorÃ© par git)
â”‚   â”œâ”€â”€ scraped_data.json        # DonnÃ©es scrapÃ©es
â”‚   â”œâ”€â”€ faiss_index.bin          # Index vectoriel FAISS
â”‚   â”œâ”€â”€ faiss_mapping.json       # Mapping des chunks
â”‚   â”œâ”€â”€ processed_documents.json # MÃ©tadonnÃ©es des documents
â”‚   â”œâ”€â”€ archive_*/               # Sauvegardes automatiques
â”‚   â”œâ”€â”€ leads/                   # DonnÃ©es des leads
â”‚   â””â”€â”€ uploads/                 # Documents uploadÃ©s
â”‚
â”œâ”€â”€ Back/
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ esilv-smart-assistant-xxxxx.json  # Credentials GCP (Ã  placer - ignorÃ© par git)
â”‚       â”œâ”€â”€ admin_indexer.py                  # Module d'indexation
â”‚       â”œâ”€â”€ document_manager.py               # Gestion des documents
â”‚       â”œâ”€â”€ leads_manager.py                  # Gestion des leads
â”‚       â”‚
â”‚       â”œâ”€â”€ agents/                           # Agents conversationnels
â”‚       â”‚   â”œâ”€â”€ orchestrator.py              # Orchestrateur principal
â”‚       â”‚   â”œâ”€â”€ rag_agent.py                 # Agent RAG
â”‚       â”‚   â”œâ”€â”€ contact_agent.py             # Agent de contact
â”‚       â”‚   â””â”€â”€ base_agent.py                # Classe de base
â”‚       â”‚
â”‚       â””â”€â”€ rag/                             # SystÃ¨me RAG
â”‚           â”œâ”€â”€ scraper.py                   # Script de scraping
â”‚           â”œâ”€â”€ indexer.py                   # Script d'indexation
â”‚           â”œâ”€â”€ chunker.py                   # DÃ©coupage de texte
â”‚           â””â”€â”€ rag.py                       # Recherche vectorielle
â”‚
â”œâ”€â”€ Front/
â”‚   â”œâ”€â”€ streamlit_app.py         # Interface utilisateur
â”‚   â”œâ”€â”€ DEPLOYMENT.md            # Guide de dÃ©ploiement
â”‚   â””â”€â”€ assets/                  # Ressources visuelles
â”‚
â””â”€â”€ admin_pages/                 # Pages d'administration
    â”œâ”€â”€ auth.py                  # Authentification admin
    â”œâ”€â”€ document_management.py   # Gestion des documents
    â””â”€â”€ leads_management.py      # Gestion des leads
```

## ğŸ”„ Mise Ã  jour des donnÃ©es

### Re-scraper le site

Pour mettre Ã  jour les donnÃ©es du site ESILV :

```bash
python Back/app/rag/scraper.py
```

### Re-indexer aprÃ¨s scraping

AprÃ¨s avoir re-scrapÃ©, il faut re-indexer :

```bash
python Back/app/admin_indexer.py
```

### Via l'interface admin

L'application Streamlit inclut une interface d'administration accessible depuis le menu latÃ©ral qui permet de :
- Re-scraper et re-indexer directement
- GÃ©rer les documents uploadÃ©s
- Consulter les leads/contacts

## ğŸ§ª Tests

Pour tester le systÃ¨me RAG :

```bash
python Back/app/rag/main.py
```

## âš™ï¸ Configuration avancÃ©e

### ModÃ¨les Vertex AI disponibles

Dans le fichier `.env`, vous pouvez changer le modÃ¨le utilisÃ© :
- `gemini-2.0-flash-exp` (par dÃ©faut, le plus rapide)
- `gemini-1.5-pro`
- `gemini-1.5-flash`

### ParamÃ¨tres de chunking

Dans le fichier `.env` :
- `CHUNK_SIZE` : Taille des chunks de texte (dÃ©faut: 1000)
- `CHUNK_OVERLAP` : Chevauchement entre chunks (dÃ©faut: 200)

### ModÃ¨le d'embeddings

Le modÃ¨le `paraphrase-multilingual-MiniLM-L12-v2` est utilisÃ© par dÃ©faut pour les embeddings.
Modifiable dans `Back/app/admin_indexer.py` (variable `MODEL_NAME`).

## ğŸ”’ SÃ©curitÃ©

**Fichiers sensibles ignorÃ©s par git :**
- `.env` : Variables d'environnement et mots de passe
- `Back/app/*.json` : Credentials GCP
- `data/` : Toutes les donnÃ©es gÃ©nÃ©rÃ©es
- `__pycache__/` : Fichiers Python compilÃ©s

**âš ï¸ Ne JAMAIS commiter :**
- Les credentials GCP (fichiers .json)
- Le fichier .env
- Les donnÃ©es scrapÃ©es ou indexÃ©es

## ğŸ› RÃ©solution des problÃ¨mes

### Erreur : "GOOGLE_APPLICATION_CREDENTIALS not found"

VÃ©rifier que :
1. Le chemin dans `.env` est correct et ABSOLU
2. Le fichier JSON existe bien Ã  cet emplacement
3. Les permissions de lecture sont correctes

### Erreur lors du scraping

VÃ©rifier :
1. La connexion internet
2. L'URL dans `.env` est accessible
3. Le site cible n'a pas changÃ© sa structure

### Erreur lors de l'indexation

VÃ©rifier :
1. Le fichier `data/scraped_data.json` existe
2. Il contient des donnÃ©es valides
3. Suffisamment d'espace disque disponible

## ğŸ“ Licence

[Ã€ complÃ©ter selon votre licence]
